\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.show='asis')
options(replace.assign=TRUE,width=72)
@

\title{Stat 154 Problem Set One}


\author{Jinze Gu SID:24968967}


\maketitle

<<Problem1, fig.height=5,fig.width=5.5, fig.align='center'>>=
# Problem One
# Data Cleaning:
setwd("/Volumes/有能出没/Stat 154/Stat 154")
meta <- read.csv("stock.csv")
# To calculate daily returns, I only need price, so I extract the data with price,
# date and company name.
stock <- data.frame("date" = meta$date, "COMP" = meta$COMNAM, "PRC" = meta$PRC)

# Just remove the raw data since it takes too much memory.
rm(meta); gc()
compname <- levels(factor(stock$COMP))

# f is a function that extract the price of the same company with 1342 price records. It turns out that there are 422 companies with 1342 sotck price records.
f <- function(x){
  if(length(stock$PRC[stock$COMP == compname[x]]) == 1342){
    return(stock$PRC[stock$COMP == compname[x]])
  }
}
stock_l <- sapply(c(1:length(compname)), f)

# g is a function that extract the name of the company with 1342 price records.
g <- function(x){
  if(length(stock$PRC[stock$COMP == compname[x]]) == 1342){
    return(compname[x])
  }
}
names <- sapply(c(1:length(compname)), g)

# Now I construct the price_matrix with price record in different date as row and 
# with different company as column.
price_ma <- matrix(unlist(stock_l), nrow = 1342)
colnames(price_ma) <- unlist(names)

# Problem One 1).
# I calculated the daily return matrix based on price_ma, and I transformed daily return matrix into percentage representation so that I can avoid numerical issue.
daily_return <- (price_ma[-1,]-price_ma[1:1341,])/price_ma[1:1341,] * 100

# The I do PCA to the daily return matrix with scaled value.
prin_stock <- prcomp(daily_return, rtex = TRUE, scale = TRUE)
screeplot(prin_stock)
explain_var_stock <- sapply(1:422, function(i) sum(prin_stock$sdev[1:i]^2)/sum(prin_stock$sdev^2))
# I can check the variance contribution of the first several principal components. Besides, it is reasonable to test how many principal components we want to leave in order to keep sat 80% of information
head(explain_var_stock)
sum(explain_var_stock < 0.8)

# Interpretation of first few principal component loadings:
hist(prin_stock$rotation[,1], main = "First Principal Direction")
max(prin_stock$rotation[,1]); min(prin_stock$rotation[,1])
hist(prin_stock$rotation[,2], main = "Second Principal Direction")
max(prin_stock$rotation[,2]); min(prin_stock$rotation[,2])
hist(prin_stock$rotation[,3], main = "Third Principal Direction")
max(prin_stock$rotation[,3]); min(prin_stock$rotation[,3])

# Comment: Based on the principal loadings, in the first three principal component loadings, most of the variables(companies in this case) have similar variance. It means that each variable are equally important in accounting for the variability in the PC.

# Besides, I plotted the projected data on the first and second principal component direction. I don't think there is an obvious clustering.
plot(prin_stock$x[,1], prin_stock$x[,2], xlab = "First Principal Component", ylab = "Second Principal Component")

# Compar the plot that I only use the projected data in the first and second PC direction rather than using all of the eigenvectors to project the data.
prin_stock <- prcomp(t(daily_return), rtex = TRUE, scale = TRUE)
stock_proj <- t(prin_stock$rotation[,1:2])%*%scale(daily_return)
plot(t(stock_proj), main = "Projected data in the first and second PCs")

# Comment: I did not see any obviously great clustering.

# Problem One 2).
# If I use all 422 companies to do hieararchical clustering, then I have the following graph.
dist_stock <- dist(t(daily_return), method = "manhattan")
hc_stock <- hclust(dist_stock, method = "complete")
plclust(hc_stock, labels = FALSE)

# Comment: I chose manhattan metric since I believe daily return should be equally weighted for everyday and it is reasonable to calculate the absolute difference between daily return rather than the euclidean distance. From the plot, I can see that those companies can be classified as different groups based on their variance of daily return.

# I used only 10 companies which would generate a better plot.
sample <- c(1,40,60,114,210,275,89,320,170,413)
dist_stock_sub <- dist(t(daily_return[,sample]), method = "manhattan")
hc_stock_sub <- hclust(dist_stock_sub, method = "complete")
plclust(hc_stock_sub, labels = NULL)


@
Problem One 1):
Interpretation of first few vectors of PC loadings:
Comment: Based on the principal loadings, in the first three principal component loadings, most of the variables(companies in this case) have similar variance. It means that each variable are equally important in accounting for the variability in the PC.

Problem One 2):
Comment: I chose manhattan metric to do hclustering since I believe daily return should be equally weighted for everyday and it is reasonable to calculate the absolute difference between daily return rather than the euclidean distance. From the plot, I can see that those companies can be classified as different groups based on their variance of daily return.



\end{document}